如何对使用深度策略抓取数据时出现错误的url进行重新抓取
--------------------------------------------

假设，目前需要爬取数个链接深度为10的url，并且该网站存在连接失败与连接被删除等问题。

可能存在的问题:

1. 在访问到中间某一层时，当访问下一个url时访问失败。
2. 在访问到某一层时，刚刚提取到的下一个url已经被该网站删除了。
3. 多个url指向的下一层url可能相同。
4. 进程被迫中断。

需要解决的问题:

1. 超时重连
2. 如何进行回溯并重新提取新的url
3. url去重
4. 断点续传

解决方案1
~~~~~~~~~

准备一个栈结构用来记录深度访问时的整条url链；三个 `bloom filter` 分别用来记录访问成功的url，`connection lost` 状态的url，以及 
`404 not found` 的url；三个 `link list` 分别用来记录访问成功的url, `connection lost` 状态的url，以及`404 not found` 的url；
       
基本流程::

    1. 添加一个初始url
    2. 访问该url,并获取对应的响应状态码(200, 404, ..)
    。。出现新的问题
    
    
   
   
